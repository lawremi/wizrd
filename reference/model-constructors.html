<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Agent Constructors — model-constructors • wizrd</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Agent Constructors — model-constructors"><meta property="og:description" content="Convenience constructors for Agent objects based on different
  backends, such as Ollama or OpenAI. Users should call one of these
  functions to gain access to an LLM. For the local backends, every
  attempt is made to install the model and even the backend in some
  cases, so that users can call these functions in a script
  and expect the script to run anywhere, more or less reproducibly."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">


    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">wizrd</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../articles/wizrd.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/lawremi/wizrd" class="external-link">
    <span class="fa fa-github fa-lg"></span>

  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Agent Constructors</h1>

    <div class="hidden name"><code>model-constructors.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Convenience constructors for Agent objects based on different
  backends, such as Ollama or OpenAI. Users should call one of these
  functions to gain access to an LLM. For the local backends, every
  attempt is made to install the model and even the backend in some
  cases, so that users can call these functions in a script
  and expect the script to run anywhere, more or less reproducibly.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">ollama_agent</span><span class="op">(</span><span class="va">name</span>, pull <span class="op">=</span> <span class="cn">NA</span>, server <span class="op">=</span> <span class="fu"><a href="ollama_server.html">ollama_server</a></span><span class="op">(</span><span class="op">)</span>, <span class="va">...</span><span class="op">)</span></span>
<span><span class="fu">llama_cpp_agent</span><span class="op">(</span><span class="va">path</span>, mode <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"chat"</span>, <span class="st">"embedding"</span><span class="op">)</span>, alias <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                server_path <span class="op">=</span> <span class="cn">NULL</span>, <span class="va">...</span><span class="op">)</span></span>
<span><span class="fu">openai_agent</span><span class="op">(</span>name <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>, <span class="va">...</span><span class="op">)</span></span>
<span><span class="fu">azure_openai_agent</span><span class="op">(</span>name <span class="op">=</span> <span class="st">"gpt-4o"</span>, url <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">getOption</a></span><span class="op">(</span><span class="st">"wizrd_azure_openai_url"</span><span class="op">)</span>,</span>
<span>                   <span class="va">...</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <p></p>
<dl><dt id="arg-name">name<a class="anchor" aria-label="anchor" href="#arg-name"></a></dt>
<dd><p>Name of the model as identified by the backend</p></dd>

  <dt id="arg-pull">pull<a class="anchor" aria-label="anchor" href="#arg-pull"></a></dt>
<dd><p>Whether to pull the model from Ollama's repository. If <code>NA</code>
    (the default), prompt the user, except when the session is
    non-interactive, in which case <code>TRUE</code> is assumed.</p></dd>

  <dt id="arg-server">server<a class="anchor" aria-label="anchor" href="#arg-server"></a></dt>
<dd><p>OllamaServer object. By default, looks for a server running on
    localhost on the default port. If no server is found, attempts to
    start the server. Note that if this ends up starting the server, the
    server process will be killed when the returned object goes out of
    scope. Thus, it is safest to start the server explicitly using
    <code><a href="ollama_server.html">ollama_server</a></code> and keep a reference around (or just run
    <code class="command">ollama serve</code> in a shell).</p></dd>

  <dt id="arg-path">path<a class="anchor" aria-label="anchor" href="#arg-path"></a></dt>
<dd><p>A string pointing to a
    <a href="https://github.com/Mozilla-Ocho/llamafile" class="external-link">llamafile</a> or a
    file containing weights in a format compatible with
    llama.cpp. This can be a simple path on the file system, a URL or
    the name of an Ollama model. If it resembles a URL, the file is
    downloaded and cached in a standard place (see
    <code><a href="https://rdrr.io/r/tools/userdir.html" class="external-link">R_user_dir</a></code>). If it resembles an Ollama
    identifier, the path to the model is automatically resolved, pulling
    the model if necessary.</p></dd>

  <dt id="arg-mode">mode<a class="anchor" aria-label="anchor" href="#arg-mode"></a></dt>
<dd><p>Whether the model is to be used for <code>"chat"</code> or
    <code>"embedding"</code>. Embedding models are accessed via the llamafile
    "v2" web server, while chat models are run through the default
    llama.cpp web server.</p></dd>

  <dt id="arg-alias">alias<a class="anchor" aria-label="anchor" href="#arg-alias"></a></dt>
<dd><p>An alternative, more human-friendly name for the model, instead of
    using the <code>path</code>.</p></dd>

  <dt id="arg-server-path">server_path<a class="anchor" aria-label="anchor" href="#arg-server-path"></a></dt>
<dd><p>The path to the llama.cpp server binary. Ignored if <code>path</code> is a
    (self-contained) llamafile. Otherwise, defaults to the automatic,
    user-local installation of the llamafile utility, downloading and
    installing it if necessary and (when interactive) the user approves.</p></dd>

  <dt id="arg-url">url<a class="anchor" aria-label="anchor" href="#arg-url"></a></dt>
<dd><p>The URL of the Azure endpoint, typically ending with the hostname.</p></dd>

  <dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Model parameters, see <code><a href="LanguageModelParams.html">LanguageModelParams</a></code>.</p></dd>

</dl></div>
    <div id="commercial-providers">
    <h2>Commercial Providers</h2>
    <p>Using a commercial provider, like via <code>openai_agent</code> or
  <code>azure_openai_agent</code>, requires some configuration, depending on
  the provider:</p><dl><dt>openai_agent</dt>
<dd><p><var>OPENAI_API_KEY</var> holds the API key</p></dd>

    <dt>azure_openai_agent</dt>
<dd><p><var>AZURE_OPENAI_API_KEY</var> holds the API
      key, and <code>option(wizrd_azure_openai_url=)</code> sets the
      account-specific URL (up to the hostname).</p></dd>


</dl></div>
    <div id="issues-with-llamafile">
    <h2>Issues with llamafile</h2>
    <p>While a llamafile is supposed to work seamlessly across platforms,
  OS-specific issues may arise. Examples include:</p><ul><li><p>On Linux, issues with binary format registration may result in
      a wine popup appearing when attempting to run a llamafile.</p></li>
<li><p>On Windows, llamafiles over 4GB in size are not supported.</p></li>
</ul><p>See the <a href="https://github.com/Mozilla-Ocho/llamafile" class="external-link">llamafile
    README</a> for workarounds.</p>
    </div>
    <div id="value">
    <h2>Value</h2>
    <p>An Agent object</p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Michael Lawrence</p>
    </div>
    <div id="note">
    <h2>Note</h2>
    <p>These functions rely on an unexported lower level API that will
  eventually be exported to enable extension to additional backends.</p>
    </div>
    <div id="see-also">
    <h2>See also</h2>
    <div class="dont-index"><p><code><a href="chat.html">predict</a></code> and <code><a href="chat.html">chat</a></code>
  for interacting with the returned Agent object.</p></div>
    </div>

    <div id="ref-examples">
    <h2>Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span>    <span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">ollama_agent</span><span class="op">(</span><span class="st">"llama3.2:3b-instruct-q4_K_M"</span><span class="op">)</span> <span class="co"># or llama()</span></span></span>
<span class="r-in"><span>    <span class="va">model</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>        <span class="fu"><a href="instruct.html">instruct</a></span><span class="op">(</span><span class="st">"Answer questions about this dataset:"</span>, <span class="va">mtcars</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>        <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="st">"What is the relationship between horsepower and mpg?"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Michael Lawrence.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

      </footer></div>






  </body></html>

