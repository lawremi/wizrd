---
title: "wizrd"
author: "Michael Lawrence"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{wizrd}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
httptest2::start_vignette("http")
```

# Overview

The wizrd package exists to test the hypothesis that Large Language
Models (LLMs) can be programmed as functions and integrated with data
science tools and workflows implemented in any programming
language. To accomplish this, wizrd defines a grammar and implements a
fluent API for programming with LLMs.

# Quick start

To start chatting with a model:
```{r quickstart, results='asis'}
library(wizrd)
model <- llamafile_llama()
ans <- predict(model, "Who created R?")
ans |> strwrap() |> paste(">", text = _, collapse = "\n") |> cat()
```

The call to `llamafile_llama()` will download, cache and run the
self-contained, cross-platform llamafile binary for the llama 3.2 3B
model. Behind the scenes, it starts up a local HTTP server (based on
llama.cpp) through which it communicates with the R process. For more
general use of local LLMs, it is recommended to install Ollama and use
`ollama_agent()` to pull (if necessary) and run models with
Ollama. The `llama()` function is a convenience to run llama 3.2 3B
with Ollama. Convenience functions exist for some other common models.

The `predict()` function is the most convenient way to execute single
exchanges with a model. To maintain a context over multiple exchanges,
use the `chat()` function:
```{r chat}
ctx <- model |> chat("Who created R?") |> chat("When was it created?")
ctx
```
A useful feature of the returned Chat object is that it prints the
context in a readable format.

To extract the last output from the Chat object, call `last_output()`:
```{r last_output}
last_output(ctx)
```
The returned value can then be used in further computations.

As an exercise, try to create a readline-based chatbot interface. 
See `wizrd:::readline_chat` for one answer.

# LLMs as functions

There are three requirements for LLMs to act as functions:
1. Accept a list of input parameters, each of arbitrary type,
1. Implement a series of logical operations, potentially delegating to
   R functions, including those based on an LLM, and
1. Return an R object of a specified type and structure.

We will demonstrate how the wizrd package meets each of these
requirements in turn. We will use gpt-4o-mini for this example, in
order to support constrained output. Set the `OPENAI_API_KEY`
environment variable to your OpenAI key before running this.
```{r openai_agent}
model <- openai_agent("gpt-4o-mini", temperature = 0)
```
For reproducibility reasons, it is best to explicitly specify the
underlying model, as above, because the default will change as new
models are released.

## Parameterized input

Let's extend the above example so that it can answer the same
questions about any given programming language.

```{r glue}
parameterized_agent <- model |> prompt_as("Who created {language}?")
parameterized_agent |> chat(list(language = "R"))
```

By calling `prompt_as()`, we parameterized the model using a glue
template to accept a parameter named `language`. By passing `"R"` as
the `language`, we get the same result as in the previous section.

## Implementing logic

We can define the logic of the LLM function using natural language
instructions, inserted into the system prompt, using the `instruct()`
function:
```{r instruct}
instructed_agent <- parameterized_agent |>
    instruct("Include the name of every author.",
             "Return only names separated by commas, no extra text.")
```
The model obeys the instructions to include every contributor and only
return the actual names.

We can also provide a tool that queries for the authors of the R
language:
```{r tool}
get_R_authors <- function() {
    authors <- R.home("doc") |> file.path("AUTHORS") |> readLines()
    paste("Here are all of the R authors:", paste(authors, collapse = "\n"))
}
equipped_agent <- instructed_agent |> equip(get_R_authors)
equipped_agent |> chat(list(language = "R"))
```

## Constraining output

In order to incorporate the output into a larger program, it is often
necessary to convert the output to a more standardized and computable
object. We made some progress through our instructions above, but we
can be much more precise using the `output_as()` function:

```{r output_as}
df_agent <- equipped_agent |>
    output_as(data.frame(first_name = character(), last_name = character()))
df_agent |> predict(list(language = "R"))
```

In the above example, we use a data.frame stub to specify the format,
so the model returns a completed data.frame, with the same columns. 

We could also use an S7 object to capture the output:
```{r S7}
library(S7)
Person <- new_class("Person",
                    properties = list(first_name = class_character,
                                      last_name = class_character))
method(print, Person) <- function(x, ...)
    cat("People:", paste(x@first_name, x@last_name, collapse = ", "), "\n")
s7_agent <- equipped_agent |> output_as(Person)
s7_agent |> predict(list(language = "R"))
```

## Converting a model to an actual R function

Since the model is already behaving like a function, it is relatively
straightforward to convert it into an actual R function using the
`convert()` generic from S7:
```{r function}
model_fun <- convert(df_agent, class_function)
model_fun("R")
```

# Additional features

## Sharing LLM-based programs across languages

As shown above, we can implement LLM programs using natural language,
along with standard JSON schema for defining output and tool calling
contracts. None of that is specific to R or any other programming
language. Thus, we can share the underlying implementation across
languages.

The Langsmith Hub is a community platform for sharing prompt templates
and output schema. We can load a program from the hub by passing the
hub ID to `prompt_as()`:
```{r langsmith}
model |> prompt_as("lawremi/creators") |> predict(list(language = "R"))
```

## Retrieval Augmented Generation (RAG)

The wizrd package implements experimental functionality
for Retrieval Augmented Generation (RAG). One potentially useful
application is in the querying of R manual pages. 

### Basic example

The code below uses the `chunk()` generic to generate text chunks from
the S7 man pages. Next, it creates a TextStore that indexes those
chunks using the nomic text embedding model. It then configures the
prompt generator to query the text store for chunks that are similar
to the query. Finally, it sends the query for an example of the
`S7::new_property()` function. 

The `chunk()` utility has basic support for a number of formats,
including markdown derivatives, HTML and PDF.

Since the output is markdown, we embed it
directly in this document. 

```{r rag, results='asis'}
chunks <- chunk(tools::Rd_db("S7"))
store <- text_store(nomic(), chunks)
model <- llama() |> prompt_as(rag_from(store))
cat("#### new_property example\n")
last_message(chat(model, "new_property example"))
```

```{r end, include=FALSE}
httptest2::end_vignette()
```
