---
title: "wizrd"
author: "Michael Lawrence"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{wizrd}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
)
httptest2::start_vignette("http")
```

# Overview

The wizrd package exists to test the hypothesis that Large Language
Models (LLMs) can be programmed as functions and integrated with data
science tools and workflows implemented in any programming
language. To accomplish this, wizrd defines a grammar and implements a
fluent API for programming with LLMs.

Just a utility for pretty-printing in Rmd block quotes:
```{r pretty-print}
pretty_rmd <- function(x) {
    x |> strwrap() |> paste(">", text = _, collapse = "\n") |> cat()
}
```

# Quick start

To start analyzing data with an agentl:
```{r quickstart, results='asis'}
library(wizrd)
agent <- llamafile_llama()
predict(agent, "Describe the mtcars dataset") |> pretty_rmd()
```

The call to `llamafile_llama()` will download, cache and run the
self-contained, cross-platform llamafile binary for the llama 3.2 3B
model. Behind the scenes, it starts up a local HTTP server (based on
llama.cpp) through which it communicates with the R process. For more
general use of local LLMs, it is recommended to install Ollama and use
`ollama_agent()` to pull (if necessary) and run models with
Ollama. The `llama()` function is a convenience to run llama 3.2 3B
with Ollama. Convenience functions exist for some other common models.

The `predict()` function is the most convenient way to execute single
exchanges with an agent. To maintain a context over multiple exchanges,
use the `chat()` function:
```{r chat}
ctx <- agent |>
    chat("Describe the mtcars dataset") |>
    chat("How can I manipulate it in R?")
ctx
```
A useful feature of the returned Chat object is that it prints the
context in a readable format.

To extract the last output from the Chat object, call `last_output()`:
```{r last_output, results='asis'}
last_output(ctx) |> pretty_rmd()
```
The returned value can then be used in further computations.

As an exercise, try to create a readline-based chatbot interface. 
See `wizrd:::readline_chat` for one answer.

# LLMs as functions

There are three requirements for LLMs to act as functions:
1. Accept a list of input parameters, each of arbitrary type,
1. Implement a series of logical operations, potentially delegating to
   R functions, including those based on an LLM, and
1. Return an R object of a specified type and structure.

We will demonstrate how the wizrd package meets each of these
requirements in turn. We will use gpt-4o-mini for this example, in
order to support constrained output. Set the `OPENAI_API_KEY`
environment variable to your OpenAI key before running this.
```{r openai_agent}
agent <- openai_agent("gpt-4o-mini", temperature = 0) |>
    instruct("Answer questions about this dataset:", mtcars)
```
For reproducibility reasons, it is best to explicitly specify the
underlying model, as above, because the default will change as new
models are released.

The `instruct()` function configures the agent with a system prompt,
instructing the agent and providing basic context. In this case, we
insert the mtcars dataset verbatim as context, which the agent can
reference in its responses. The agent will now be able to answer
questions about the dataset's characteristics.

## Parameterized input

Let's extend the above example so that it can analyze any given
variable in the mtcars dataset.

```{r glue}
parameterized_agent <- agent |>
    prompt_as("Analyze the relationship between {var1} and {var2}.")
parameterized_agent |> chat(list(var1 = "mpg", var2 = "wt"))
```

By calling `prompt_as()`, we parameterized the agent using a glue
template to accept parameters named `var1` and `var2`. By passing
`"mpg"` and `"wt"` as the variables, we get an analysis of the
relationship between fuel efficiency and weight.

## Implementing logic

We can define the logic of the LLM function using natural language
instructions, inserted into the system prompt, using the `instruct()`
function:
```{r instruct}
instructed_agent <- parameterized_agent |>
    instruct("Answer questions about this dataset:", mtcars,
             "When comparing variables, calculate their correlation.")
chat(instructed_agent, list(var1 = "mpg", var2 = "wt"))
```
The agent will now provide a more structured analysis including
correlation, interpretation, and visualization suggestions. However,
it is not able to carry out the correlation calculation.

To solve this, we can provide a tool that performs the actual
correlation calculation:
```{r tool}
calculate_correlation <- function(var1, var2) {
    cor(mtcars[[var1]], mtcars[[var2]])
}
equipped_agent <- instructed_agent |> equip(calculate_correlation)
equipped_agent |> chat(list(var1 = "mpg", var2 = "wt"))
```

## Constraining output

In order to incorporate the output into a larger program, it is often
necessary to convert the output to a more standardized and computable
object. We can use the `output_as()` function to structure our
analysis results:

```{r output_as}
# Return the correlation as a single number
equipped_agent |>
    output_as(S7::class_numeric) |>
    predict(list(var1 = "mpg", var2 = "wt"))

# Return a filtered subset of mtcars, which serves as the prototype
filtered_agent <- agent |>
    output_as(mtcars)
filtered_agent |>
    predict("Cars with mpg > 20")

# Return a summary data.frame
summary_agent <- agent |>
    output_as(data.frame(
        cyl = integer(),
        avg_mpg = numeric(),
        avg_hp = numeric()
    ))
summary_agent |>
    predict("Average mpg and hp by number of cylinders.")

# Using class_data.frame for more general output
agent |>
    output_as(S7::class_data.frame) |>
    predict("Top 5 most fuel efficient cars including mpg, hp, and wt.")
```

In the above examples, we demonstrate different ways to structure the output:
1. Using an existing data.frame (`mtcars`) as a template
2. Using a data.frame stub with specific columns
3. Using `class_data.frame` for more general output

The agent will return properly structured data.frames that can be used
directly in further analysis or visualization.

Note that `output_as()` supports any S7 class as a constraint, not
just data.frames.  This allows for type-safe conversion of agent
outputs into any R object structure defined using S7.

## Converting an agent to an actual R function

Since the agent is already behaving like a function, it is relatively
straightforward to convert it into an actual R function using the
`convert()` generic from S7:
```{r function}
# Convert the filtered agent to a function
filter_cars <- S7::convert(filtered_agent, S7::class_function)
filter_cars("Cars with mpg > 30")
```

# Additional features

## Model Context Protocol (MCP)

The wizrd package implements a client for the Model Context Protocol
(MCP), which enables constructing agents from shared ingredients,
including tools, prompts, and resources.

Here's an example of using MCP to interact with a data analysis server:

```{r mcp}
# Start the data analysis server
mcp_args <- c("fastmcp", "run",
              system.file("mcp", "data_server.py", package = "wizrd"))
server <- exec_mcp_server("uvx", mcp_args)

# Create an MCP session
session <- connect_mcp(server)

# List available tools
tools <- tools(session)

# Tools are just ordinary R functions
tools$get_mean(mtcars$mpg)

# Equip a agent with the MCP tools and analyze the data
mcp_agent <- agent |>
    output_as(S7::class_numeric) |>
    equip(tools)
predict(mcp_agent, "What is the mean fuel efficiency in the mtcars dataset?")
```

The MCP protocol provides a standardized way to:
1. Discover available tools, prompts, and resources
2. Call tools with structured arguments
3. Access and use predefined prompts
4. Handle resources and templates

This makes it easier to work with different agent implementations while
maintaining a consistent interface in R.

## Retrieval Augmented Generation (RAG)

The wizrd package implements experimental functionality
for Retrieval Augmented Generation (RAG). One potentially useful
application is in the querying of R manual pages. 

### Querying Rd

The code below uses the `chunk()` generic to generate text chunks from
the S7 man pages. Next, it creates a TextStore that indexes those
chunks using the nomic text embedding model. It then configures the
prompt generator to query the text store for chunks that are similar
to the query. Finally, it sends the query for an example of the
`S7::new_property()` function. 

The `chunk()` utility has basic support for a number of formats,
including markdown derivatives, HTML and PDF.

Since the output is markdown, we embed it
directly in this document. 

```{r rag}
chunks <- chunk(tools::Rd_db("S7"))
store <- text_store(nomic(), chunks)
model <- llama() |> prompt_as(rag_with(store))
cat("#### new_property example\n")
last_message(chat(model, "new_property example"))
```

### Querying a data.frame

And here is an example of querying a data.frame using RAG:
```{r rag-df}
chunks <- chunk(mtcars)
store <- text_store(nomic(), chunks)
model <- llama() |> prompt_as(rag_with(store))
predict(model, "MPG of Datsun 710")
```

```{r end, include=FALSE}
httptest2::end_vignette()
```
