#+TITLE: wizrd: An LLM-based Assistant for R

* Motivation

LLMs are very good at generating code and documentation, as well as
analyzing code, for example to suggest improvements or to annotate
code with inferred types. Thus, there is a clear trend towards
integration of LLMs with IDEs, and products like GitHub Co-Pilot
support R, despite it not being one of their target languages.

One thing that sets R apart from other languages is that it is
designed for interactive analysis, conventionally through a console
interface. As the user enters code, objects are created in the global
environment, and each line of code enters into the history of the
session. By integrating an LLM directly into the R session, as opposed
to operating on a buffer of code, we can enhance the user's experience
as they interact with their data. The user could pass a data object or
function object to an LLM for an explanation or other advice. The LLM
could analyze the code that the user has entered so far and suggest
improvements or next steps.

LLMs can also help with the two fundamental R workflows that often
interact: software development and data analysis. Both often start
with prototyping at the console and then move to persisting code in a
buffer. The software workflow then proceeds to refactor the code into
reusable functions and eventually to incorporate the functions into a
package, complete with documentation. The analysis workflow moves from
a script to a literate programming document, usually an Rmd
file. Generative AI can automate many of these tasks, including
refactoring and generation of Rd and Rmd files.

Alternatively, the user might prefer a notebook style interface, which
would benefit from the same features, only more incrementally.

* Design

There are several components to the design:
 * An R interface to something like LLaMA.cpp, so that we can interact
   with models from R,
 * The agents built on the models, which need to be adapted for
   specific use cases,
 * The user-friendly interface to models and agents, allowing the user
   to engage in dialog directly from the R console, index the R
   workspace, as well as develop agent-based programs.
 * IDE integration, probably by extending the Ark LSP by Lionel Henry.

** Model interface

We will define an abstraction layer for sending prompts to and
receiving replies from an LLM. The user will instantiate a Model using
the backend-specific API. That Model will use default prompts and
concern itself only with communication to the backend. If the user
wants to customize the prompts to a specific to a task, they can
modify the prompt parameters. To optimize support for specific species
of models, the user can define a Model subclass that wraps the base
Model. Dispatch will then generate Model-specific prompts (see below)
where desired.

LLaMA.cpp seems like the ideal basis for an R interface to local LLMs,
because it is conveniently implemented in C/C++ and has a strong
ecosystem around it, including LlamaIndex for RAG, fine-tuning etc,
although it is only available from Python.

There is a barebones R interface to LLaMA.cpp called [[https://github.com/coolbutuseless/rllama][rllama]]. It is
very basic, not released anywhere, directly embeds/copies code from
the llama.cpp project, and hasn't been updated in a year. It would be
easier to implement something like rllama after the release of the
[[https://github.com/ggerganov/llama.cpp/issues/5215][llamax]] C library, which will wrap the C++ constructs in a high-level
API.

However, a simpler approach might be to leverage the llama.cpp [[https://github.com/ggerganov/llama.cpp/tree/master/examples/server][HTTP
server]]. Its REST interface implements the OpenAI API, so if we
implemented a client for that, we would also support other servers,
such as Chat GPT itself. The server has tons of features, including
key-based access and automatic downloading of models from Hugging
Face.

Advantages of the REST approach include:
 * No native code, just a dependency on a web client,
 * Client/server architecture enables computing on a remote host, such
   as a shared/cloud node with more resources (GPUs), enabling the use
   of larger, more performant models.

The R package could automate installation of the web server, as
binaries for most platforms are provided from GitHub.

There is a good [[https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/][overview]] of large language model abstractions (in
Python). Related work in R includes:
 * The [[https://github.com/jcrodriguez1989/chatgpt/tree/main][chatgpt]] package, which is hard-coded against the OpenAI
   endpoint and its GPT 3.5 model,
 * The [[https://cran.r-project.org/web/packages/ollamar/index.html][ollamar]] package, which is similar to chatgpt except for ollama,
 * The [[https://github.com/JBGruber/rollama][rollama]] package, which converts the R console into a chat
   interface based on ollama (and side effects),
 * The [[https://github.com/JamesHWade/gpttools][gpttools]] package, which a simple chat abstraction over many
   popular models, but without using OOP,
 * The [[https://github.com/MichelNivard/gptstudio][gptstudio]] package, which integrates the same models as gpttools
   into RStudio, but for some reason does not leverage gpttools, even
   though they share authors.

** Model development

The package will enable users to *interactively* specialize models to
a task. We consider off-line training approaches as out of
scope. Thus, our focus is on epi-training (specializing on top of the
model weights), where EPI is an acronym:
 * Equipping :: making tools available to the model
 * Prompting :: providing effective instructions to the model
 * Indexing :: making additional knowledge efficiently accessible to
   the model.

Agents will perform tasks associated with two different activities:
software development and data analysis.

*** Software development

Tasks include:
 * Generate code, documentation and reports,
 * Explain, review and annotate code, including type inference,
   suggestions for improvement, etc, potentially by generating an Rmd,
 * Refactor code, such as converting non-standard tidyverse code to
   standard R code, improving its reusablility, and
 * Suggest the next steps of an analysis or software project.
   
We could consider fine-tuning or indexing (RAG) models using data
sources like:
 * The CRAN, Bioconductor and GitHub corpuses of vignettes and
   documentation, including artifacts on GitHub that are not packaged,
 * The [[https://zenodo.org/records/4091818][type tracing dataset]] from Jan Vitek's group.

As a point of reference, GitHub contains ~768k Rmd files.

*** Data Analysis

Tasks include:
 * Data and results discovery :: Based on context and/or a user query,
   identify relevant data/results and reference them while explaining
   their relevance and providing code for importing them.
 * Data import :: Identify the structure of a data source, like a CSV
   file, web site, REST API or database, and generate an importer
   function based on user intent, which could be indicated by a
   prototype of the imported object, like a data.frame stub.
 * Data processing :: Identify the structure of the input data,
   typically a data.frame, and generate a function to map it to
   another data structure/state, based on user instructions and/or a
   description of the desired result.
 * Summary statistics :: Generate code that automatically summarizes
   the variables in a dataset and their relationships, based on
   context and/or a query.
 * Visualization :: Generate code that creates visualizations, based
   on specific instructions, generic approaches or patterns discovered
   by numerical means, in order to check for anomalies, explore
   patterns and perform visual inference.
 * Statistical modeling :: Generate advice and/or code for testing
   hypotheses and finding patterns using statistical / ML models.
 * Report generation :: Generate Rmd reports using an analysis
   transcript and/or user instructions.
 * Publishing of results :: Generate code for publishing a result and
   descriptive metadata, including provenance, to a database or
   versison control system based on context and/or user instructions.

Some general principles:
 * Decoupling of code generation and evaluation :: Every task should
   have a corresponding agent that generates code, ideally in a
   function, without evaluating it, because there are multiple
   potential uses of code, such as giving the user the chance to
   understand, modify and integrate it.
 * Consistency in input :: Each task should consider direct
   instructions while also taking into account the broader context,
   such as the chat history, contents of the global environment and/or
   the history of the current R session.
   
** User interface

The user interface would represent the agent with a single object
(like =wizrd=) exported by the package. The =print()= method on the
object would prompt the user for input using =readline()=, so the user
just has to enter "wizrd" for help.

The model would have access to the history of the session, using the
histry package, as well as the objects visible in the search path,
loaded help files, the list of installed packages, etc. Optionally,
the agent could also hook into errors in order to offer immediate
assistance. It could also profile the user's code behind the scenes
and suggest optimizations if the user is spending a lot of time
waiting. Ideally in a way that is less annoying than Clippy from MS
Office.

Besides the direct chat interface, the package will provide
convenience functions for common tasks like generating Rd for a
function. Each function will perform one or more tasks. Each task will
be represented by a generic, so that we can rely on dispatch to
adapt the Model to each task based on its type.

The general logic for any task will be:
1. Use dispatch to determine the optimal prompt given a particular
   Task and Model,
2. Send the prompt to the Model, which returns the response,
3. Interpret the response to finalize execution of the task.
