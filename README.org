#+TITLE: wizrd: An LLM-based Assistant for R

* Motivation

LLMs are very good at generating code and documentation, as well as
analyzing code, for example to suggest improvements or to annotate
code with inferred types. Thus, there is a clear trend towards
integration of LLMs with IDEs, and products like GitHub Co-Pilot
support R, despite it not being one of their target languages.

One thing that sets R apart from other languages is that it is
designed for interactive analysis, conventionally through a console
interface. As the user enters code, objects are created in the global
environment, and each line of code enters into the history of the
session. By integrating an LLM directly into the R session, as opposed
to operating on a buffer of code, we can enhance the user's experience
as they interact with their data. The user could pass a data object or
function object to an LLM for an explanation or other advice. The LLM
could analyze the code that the user has entered so far and suggest
improvements or next steps.

LLMs can also help with the two fundamental R workflows that often
interact: software development and data analysis. Both often start
with prototyping at the console and then move to persisting code in a
buffer. The software workflow then proceeds to refactor the code into
reusable functions and eventually to incorporate the functions into a
package, complete with documentation. The analysis workflow moves from
a script to a literate programming document, usually an Rmd
file. Generative AI can automate many of these tasks, including
refactoring and generation of Rd and Rmd files.

Alternatively, the user might prefer a notebook style interface, which
would benefit from the same features, only more incrementally.

* Design

There are several components to the design:
 * An R interface to something like LLaMA.cpp, so that we can interact
   with models from R,
 * The model itself, which needs to be fine-tuned and/or indexed for
   this specific use case, and exist on the user's machine,
 * The user-friendly interface to the model, allowing the user to
   engage in dialog directly from the R console.
 * IDE integration, probably by extending the Ark LSP by Lionel Henry.

** Model interface

We will define an abstraction layer for sending prompts to and
receiving replies from an LLM. The user will instantiate a Model using the
backend-specific API. That Model will use default prompts and concern
itself only with communication to the backend. If the user wants to
customize the prompts to a specific Model, they can define a Model
subclass that wraps the base Model. Dispatch will then generate
Model-specific prompts (see below) where desired.

LLaMA.cpp seems like the ideal basis for an R interface to LLMs,
because it is conveniently implemented in C/C++ and has a strong
ecosystem around it, including LlamaIndex for RAG, fine-tuning etc,
although it is only available from Python.

There is a barebones R interface to LLaMA.cpp called [[https://github.com/coolbutuseless/rllama][rllama]]. It is
very basic, not released anywhere, directly embeds/copies code from
the llama.cpp project, and hasn't been updated in a year. It would be
easier to implement something like rllama after the release of the
[[https://github.com/ggerganov/llama.cpp/issues/5215][llamax]] C library, which will wrap the C++ constructs in a high-level
API.

However, a simpler approach might be to leverage the llama.cpp [[https://github.com/ggerganov/llama.cpp/tree/master/examples/server][HTTP
server]]. Its REST interface implements the OpenAI API, so if we
implemented a client for that, we would also support other servers,
such as Chat GPT itself. The server has tons of features, including
key-based access and automatic downloading of models from Hugging
Face.

Advantages of the REST approach include:
 * No native code, just a dependency on a web client,
 * Client/server architecture enables computing on a remote host, such
   as a shared/cloud node with more resources (GPUs), enabling the use
   of larger, more performant models.

The R package could automate installation of the web server, as
binaries for most platforms are provided from GitHub.

There is a good [[https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/][overview]] of large language model abstractions (in
Python). Related work in R includes:
 * The [[https://github.com/jcrodriguez1989/chatgpt/tree/main][chatgpt]] package, which is hard-coded against the OpenAI
   endpoint and its GPT 3.5 model,
 * The [[https://cran.r-project.org/web/packages/ollamar/index.html][ollamar]] package, which is similar to chatgpt except for ollama,
 * The [[https://github.com/JBGruber/rollama][rollama]] package, which converts the R console into a chat
   interface based on ollama (and side effects),
 * The [[https://github.com/JamesHWade/gpttools][gpttools]] package, which a simple chat abstraction over many
   popular models, but without using OOP,
 * The [[https://github.com/MichelNivard/gptstudio][gptstudio]] package, which integrates the same models as gpttools
   into RStudio, but for some reason does not leverage gpttools, even
   though they share authors.

It would be interesting to support tool calling on local models. Some
models [[https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/][support]] adhoc tool calling via system prompts.

** Model development

Things we want the model to be able to do:
 * Generate code, documentation and reports,
 * Explain, review and annotate code, including type inference,
   suggestions for improvement, etc, potentially by generating an Rmd,
 * Refactor code, such as converting non-standard tidyverse code to
   standard R code, improving its reusablility, and
 * Suggest the next steps of an analysis or software project.
   
We could consider fine-tuning or indexing (RAG) the model using data
sources like:
 * The CRAN, Bioconductor and GitHub corpuses of vignettes and
   documentation, including artifacts on GitHub that are not packaged,
 * The [[https://zenodo.org/records/4091818][type tracing dataset]] from Jan Vitek's group.

As a point of reference, GitHub contains ~768k Rmd files.

A good model to start with is the [[https://ollama.com/mannix/deepseek-coder-v2-lite-instruct][4 bit quantization]] of
=deepseek-coder-v2-lite-instruct=. The full model seems to work as
well as GPT4o in simple testing. Both seem capable at the above tasks,
so it is not clear whether fine-tuning will be necessary.

However, we do want to enable extending the system to new
models. Besides pointing to the model representation, a model
extension will also want to provide customized prompts for each type
of interaction. This behavior will be implemented by extending the
Model class (see below).

** User interface

The user interface would represent the agent with a single object
(like =wizrd=) exported by the package. The =print()= method on the
object would prompt the user for input using =readline()=, so the user
just has to enter "wizrd" for help.

The model would have access to the history of the session, using the
histry package, as well as the objects visible in the search path,
loaded help files, the list of installed packages, etc. Optionally,
the agent could also hook into errors in order to offer immediate
assistance. It could also profile the user's code behind the scenes
and suggest optimizations if the user is spending a lot of time
waiting. Ideally in a way that is less annoying than Clippy from MS
Office.

Besides the direct chat interface, the package will provide
convenience functions for common tasks like generating Rd for a
function. Each function will perform one or more tasks. Each task will
be represented by a class, so that we can rely on dispatch to
customize the generation of prompts for each Task and Model
combination (and maybe also the target object, which might be
a function, environment, histry object, etc).

The general logic for any task will be:
1. Use dispatch to determine the optimal prompt given a particular
   Task and Model,
2. Send the prompt to the Model, which returns the response,
3. Interpret the response to finalize execution of the task.

The model could request information on an object in the R session
using a protocol like {R_get X}. A prompt for that might be: "If I ask
you for help with a function, and you don't know the code, ask for it
using {R_get X} where X is the name of the function". A more
transparent alternative would be expecting the user to use a syntax
like ={R foo}= to inline substitute the result of evaluating =foo=.
